{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "40ba9bc0-e186-4a5e-ac51-7f34610f5a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import word_wakati as wkt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "30bbdbbd-0350-45cb-ac9f-17e8d132f418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def neologd():\n",
    "    dicdir = os.popen(\"mecab-config --dicdir\").read().strip()\n",
    "    neodic = os.path.join(dicdir, \"mecab-ipadic-neologd\")\n",
    "    if (os.path.isdir(neodic)):\n",
    "        return neodic\n",
    "    return None\n",
    "\n",
    "neologd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cfe0fce2-e59f-4fa2-a873-504c4d93f0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Janome\n",
      "['拡張', '現実', '感', 'ため', '物体', '奥行き', '考慮', 'する', '陰', '面', '消去']\n",
      "MeCab with ipadic-neologd\n",
      "['拡張現実', '感', 'ため', '物体', '奥行き', '考慮', 'した陰', '面', '消去']\n"
     ]
    }
   ],
   "source": [
    "pos = ['名詞','動詞','形容詞']\n",
    "mecab = wkt.create_parser(worker='mecab', parts_of_speech=pos)\n",
    "janome = wkt.create_parser(worker='janome', parts_of_speech=pos)\n",
    "text = '拡張現実感のための実物体の奥行きを考慮した陰面消去'\n",
    "rs_j = wkt.word_seq(text,parser=janome)\n",
    "rs_m = wkt.word_seq(text,parser=mecab)\n",
    "print('Janome')\n",
    "print(rs_j)\n",
    "print('MeCab with ipadic-neologd')\n",
    "print(rs_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c49b6b05-76d2-4ddc-af9b-5837de292515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Janome\n",
      "['研究', '室', '配属', '学生', '研究', '室', '理解', '深める', '情報', '共有', 'システム']\n",
      "MeCab with ipadic-neologd\n",
      "['研究室', '配属', '学生', '研究室', '理解', '深める', '情報共有', 'システム']\n"
     ]
    }
   ],
   "source": [
    "text = '研究室配属における学生の研究室に対する理解を深める情報共有システム'\n",
    "rs_j = wkt.word_seq(text,parser=janome)\n",
    "rs_m = wkt.word_seq(text,parser=mecab)\n",
    "print('Janome')\n",
    "print(rs_j)\n",
    "print('MeCab with ipadic-neologd')\n",
    "print(rs_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9502030b-d7e1-49a4-b094-b5db15816681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Janome\n",
      "['人', '感', 'センサー', '利用', 'する', '乗降', '客数', '計測', 'Android', 'アプリケーション', '開発']\n",
      "MeCab with ipadic-neologd\n",
      "['人感センサー', '利用', 'する', '乗降客数', '計測', 'Android', 'アプリケーション', '開発']\n"
     ]
    }
   ],
   "source": [
    "text ='人感センサーを利用した乗降客数計測Androidアプリケーションの開発'\n",
    "rs_j = wkt.word_seq(text,parser=janome)\n",
    "rs_m = wkt.word_seq(text,parser=mecab)\n",
    "print('Janome')\n",
    "print(rs_j)\n",
    "print('MeCab with ipadic-neologd')\n",
    "print(rs_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50974892-5bf3-46d4-b8db-28062e64a841",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 英語対応\n",
    "\n",
    "# Stemming and Lemmatization of English Text in Python with NLTK\n",
    "# cf. https://www.datacamp.com/tutorial/stemming-lemmatization-python\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bfb379b-2611-4f6e-ab97-5bab62fc0654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Word--            --Stem--            \n",
      "program             program             \n",
      "programming         program             \n",
      "programer           program             \n",
      "programs            program             \n",
      "programmed          program             \n"
     ]
    }
   ],
   "source": [
    "# Initialize Python porter stemmer\n",
    "ps = PorterStemmer()\n",
    "# Example inflections to reduce\n",
    "example_words = [\"program\",\"programming\",\"programer\",\"programs\",\"programmed\"]\n",
    "\n",
    "# Perform stemming\n",
    "print(\"{0:20}{1:20}\".format(\"--Word--\",\"--Stem--\"))\n",
    "for word in example_words:\n",
    "    print (\"{0:20}{1:20}\".format(word, ps.stem(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c720b946-de32-4f1d-bbcc-6955a219a284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f5c8d1f-ba60-42c1-97c4-cc942c25eb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Word--            --Stem--            \n",
      "Python              python              \n",
      "programmers         programm            \n",
      "often               often               \n",
      "tend                tend                \n",
      "like                like                \n",
      "programming         program             \n",
      "in                  in                  \n",
      "python              python              \n",
      "because             becaus              \n",
      "its                 it                  \n",
      "like                like                \n",
      "english             english             \n",
      "We                  we                  \n",
      "call                call                \n",
      "people              peopl               \n",
      "who                 who                 \n",
      "program             program             \n",
      "in                  in                  \n",
      "python              python              \n",
      "pythonistas         pythonista          \n"
     ]
    }
   ],
   "source": [
    "example_sentence = \"Python programmers often tend like programming in python because it's like english. We call people who program in python pythonistas.\"\n",
    "\n",
    "# Remove punctuation\n",
    "example_sentence_no_punct = example_sentence.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "# Create tokens\n",
    "word_tokens = word_tokenize(example_sentence_no_punct)\n",
    "\n",
    "# Perform stemming\n",
    "print(\"{0:20}{1:20}\".format(\"--Word--\",\"--Stem--\"))\n",
    "for word in word_tokens:\n",
    "    print (\"{0:20}{1:20}\".format(word, ps.stem(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ef3fcc-3469-4859-ad6c-9f14612b570b",
   "metadata": {},
   "source": [
    "## Note\n",
    "Some of the output words are not part of the english dictionary (i.e., \"becaus\", \"people\", and \"programm\"). Another thing to notice is that context is not taken into consideration. For instance, “programmers” is a plural noun but it was reduced down to \"program\", which can be a noun or a verb – in other words, the root words are ambiguous. This motivated context-sensitive lemmatizers to improve the performance on unseen and ambiguous words.s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3a93bd3-7a06-4a84-8b62-2d7f649d0b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WordNet lemmatizer \n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62d271e8-98e1-477d-8331-041dd360c5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Word--            --Lemma--           \n",
      "program             program             \n",
      "programming         program             \n",
      "programer           programer           \n",
      "programs            program             \n",
      "programmed          program             \n"
     ]
    }
   ],
   "source": [
    "# Initialize wordnet lemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "# Example inflections to reduce\n",
    "example_words = [\"program\",\"programming\",\"programer\",\"programs\",\"programmed\"]\n",
    "# Perform lemmatization\n",
    "print(\"{0:20}{1:20}\".format(\"--Word--\",\"--Lemma--\"))\n",
    "for word in example_words:\n",
    "    print (\"{0:20}{1:20}\".format(word, wnl.lemmatize(word, pos=\"v\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4c32951-fafe-4bb4-8faf-c781ad5bbdd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Word--            --Lemma--           \n",
      "Python              Python              \n",
      "programmers         programmers         \n",
      "often               often               \n",
      "tend                tend                \n",
      "like                like                \n",
      "programming         program             \n",
      "in                  in                  \n",
      "python              python              \n",
      "because             because             \n",
      "its                 its                 \n",
      "like                like                \n",
      "english             english             \n",
      "We                  We                  \n",
      "call                call                \n",
      "people              people              \n",
      "who                 who                 \n",
      "program             program             \n",
      "in                  in                  \n",
      "python              python              \n",
      "pythonistas         pythonistas         \n"
     ]
    }
   ],
   "source": [
    "example_sentence = \"Python programmers often tend like programming in python because it's like english. We call people who program in python pythonistas.\"\n",
    "\n",
    "# Remove punctuation\n",
    "example_sentence_no_punct = example_sentence.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "word_tokens = word_tokenize(example_sentence_no_punct)\n",
    "# Perform lemmatization\n",
    "print(\"{0:20}{1:20}\".format(\"--Word--\",\"--Lemma--\"))\n",
    "for word in word_tokens:\n",
    "    print (\"{0:20}{1:20}\".format(word, wnl.lemmatize(word, pos=\"v\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03eef4ee-5ee6-40d6-a2fa-ed7cb792d477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is in English\n"
     ]
    }
   ],
   "source": [
    "text1 = \"The two may also differ in that stemming most commonly collapses derivationally related words, whereas lemmatization commonly only collapses the different forms offical forms a lemma\"\n",
    "if text1.isascii():\n",
    "    print('Text is in English')\n",
    "else:\n",
    "    print('Text is not in English')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "22f3882a-923a-444a-9d8b-b4f6b304be82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Word--            --Lemma--           \n",
      "For                 For                 \n",
      "instance            instance            \n",
      "programmers         programmer          \n",
      "is                  be                  \n",
      "a                   a                   \n",
      "plural              plural              \n",
      "noun                noun                \n",
      "but                 but                 \n",
      "it                  it                  \n",
      "was                 be                  \n",
      "reduced             reduce              \n",
      "down                down                \n",
      "to                  to                  \n",
      "program             program             \n",
      "which               which               \n",
      "can                 can                 \n",
      "be                  be                  \n",
      "a                   a                   \n",
      "noun                noun                \n",
      "or                  or                  \n",
      "a                   a                   \n",
      "verb                verb                \n",
      "in                  in                  \n",
      "other               other               \n",
      "words               word                \n",
      "the                 the                 \n",
      "root                root                \n",
      "words               word                \n",
      "are                 be                  \n",
      "ambiguous           ambiguous           \n",
      "This                This                \n",
      "motivated           motivate            \n",
      "contextsensitive    contextsensitive    \n",
      "lemmatizers         lemmatizers         \n",
      "to                  to                  \n",
      "improve             improve             \n",
      "the                 the                 \n",
      "performance         performance         \n",
      "on                  on                  \n",
      "unseen              unseen              \n",
      "and                 and                 \n",
      "ambiguous           ambiguous           \n",
      "words               word                \n"
     ]
    }
   ],
   "source": [
    "text = \"For instance, programmers is a plural noun but it was reduced down to program, which can be a noun or a verb? in other words, the root words are ambiguous. This motivated context-sensitive lemmatizers to improve the performance on unseen and ambiguous words.\"\n",
    "text_no_punct = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "word_tokens = word_tokenize(text_no_punct)\n",
    "print(\"{0:20}{1:20}\".format(\"--Word--\",\"--Lemma--\"))\n",
    "for token in word_tokens:\n",
    "    word = wnl.lemmatize(token, pos=\"v\")\n",
    "    word = wnl.lemmatize(word, pos=\"n\")\n",
    "    word = wnl.lemmatize(word, pos=\"s\")\n",
    "    print (\"{0:20}{1:20}\".format(token, word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0141703d-e24f-4782-abe2-540b80ed02e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method lemmatize in module nltk.stem.wordnet:\n",
      "\n",
      "lemmatize(word: str, pos: str = 'n') -> str method of nltk.stem.wordnet.WordNetLemmatizer instance\n",
      "    Lemmatize `word` using WordNet's built-in morphy function.\n",
      "    Returns the input word unchanged if it cannot be found in WordNet.\n",
      "    \n",
      "    :param word: The input word to lemmatize.\n",
      "    :type word: str\n",
      "    :param pos: The Part Of Speech tag. Valid options are `\"n\"` for nouns,\n",
      "        `\"v\"` for verbs, `\"a\"` for adjectives, `\"r\"` for adverbs and `\"s\"`\n",
      "        for satellite adjectives.\n",
      "    :param pos: str\n",
      "    :return: The lemma of `word`, for the given `pos`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(wnl.lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d1f1332c-a692-4db7-b2db-bb0a4156d08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function word_tokenize in module nltk.tokenize:\n",
      "\n",
      "word_tokenize(text, language='english', preserve_line=False)\n",
      "    Return a tokenized copy of *text*,\n",
      "    using NLTK's recommended word tokenizer\n",
      "    (currently an improved :class:`.TreebankWordTokenizer`\n",
      "    along with :class:`.PunktSentenceTokenizer`\n",
      "    for the specified language).\n",
      "    \n",
      "    :param text: text to split into words\n",
      "    :type text: str\n",
      "    :param language: the model name in the Punkt corpus\n",
      "    :type language: str\n",
      "    :param preserve_line: A flag to decide whether to sentence tokenize the text or not.\n",
      "    :type preserve_line: bool\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bd0528-59fd-458b-a40e-8ba9e86d49ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
